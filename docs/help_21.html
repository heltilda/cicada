<HTML>

<HEAD>
<TITLE>Cicada Help: C implementation</TITLE>

<a href="https://heltilda.github.io/cicada/index.html">Cicada</a> ---> <a href="https://heltilda.github.io/cicada/toc.html">Online Help Docs</a> ---> <a href="https://heltilda.github.io/cicada/help_2.html">Example:  neural networks in Cicada</a>

<H1> <CENTER> C implementation </CENTER> </H1>
</HEAD>

<BODY>



<P> A neural network has two modes of operation:  computation, and training.  In computing mode, the user fixes the activity of certain input neurons, and the other neurons <i>x<sub>i</sub></i> will calculate:

<P> 

<br><P>&nbsp;&nbsp;&nbsp;<i>x<sub>i</sub></i> &nbsp;<--&nbsp; 1 / [ 1 + exp(-&sum;<i><sub>j</sub> w<sub>ij</sub> x<sub>j</sub> - b<sub>i</sub></i>) ]

<P>  Here <i>x<sub>i</sub></i> represents the activity level of neuron <i>i</i>, <i>w<sub>ij</sub></i> is the strength of the connection from neuron <i>j</i> to neuron <i>i</i>, and <i>b<sub>i</sub></i> is the neuron&rsquo;s sensitivity.  In training mode, we will update the weights and biases of each neuron using the following formula:

<P> 

<br><P>&nbsp;&nbsp;&nbsp;<i>w<sub>ij</sub> &nbsp;<--&nbsp; w<sub>ij</sub> + &eta; &middot; x<sub>i</sub> x<sub>j</sub> <P>&nbsp;&nbsp;&nbsp; b<sub>i</sub> &nbsp;<--&nbsp; b<sub>i</sub> + &eta; &middot; x<sub>i</sub></i>

<P>  The parameter &eta; determines the learning rate, which should be large enough to train quickly, but not so big that the algorithm overshoots and destabilizes the network.  This training algorithm (J. R. Movellan, Contrastive Hebbian learning in interactive networks, 1990) works for symmetric networks (<i>w<sub>ij</sub> = w<sub>ji</sub></i>) such as we will use.  As the saying goes, <i>neurons that fire together, wire together</i>.

<P> These two basic operations of a neural network are encoded by simple formulas, but since those formulas will be used many times in the course of using a network they are the time-consuming step.  We will therefore write that part of our program is C.  Notice that our outermost function has the same form as <tt>main()</tt>, but we will use a different name so as not to conflict with Cicada&rsquo;s own <tt>main()</tt> function.<P>

<P>  <u>NN.c</u>

<P> 

<P><P><tt> 

<br> #include &quot;NN.h&quot;

<br> #include &lt;math.h&gt;

<br> 

<br> 

<br> // runNetwork():&nbsp;evolves a neural network to a steady state

<br> // Takes the params:&nbsp;1 - weights; 2 - neuron activities; 3 - input; 4 - step size

<br> // (&amp; additionally, in training mode):&nbsp;5 - target output; 6 - learning rate

<br> 

<br> ccInt runNetwork(ccInt argc, char **argv)

<br> {

<br> &nbsp;&nbsp;&nbsp;neural_network myNN;

<br> &nbsp;&nbsp;&nbsp;double *inputs, step_size, *target_outputs, learning_rate;

<br> &nbsp;&nbsp;&nbsp;int i, numInputs, numOutputs;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;/* ----- set up data types, etc. ----- */

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;for (i = 0; i &lt; numInputs; i++)

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myNN.activity[i] = inputs[i];

<br> &nbsp;&nbsp;&nbsp;for (i = numInputs; i &lt; myNN.numNeurons; i++)

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myNN.activity[i] = 0;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;if ( argc == 6 )&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// i.e. if we're in training mode

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (getSteadyState(myNN, numInputs, step_size) != 0)&nbsp;return 1;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trainNetwork(myNN, -learning_rate);

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = 0; i &lt; numOutputs; i++)

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myNN.activity[numInputs + i] = target_outputs[i];

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (getSteadyState(myNN, numInputs+numOutputs, step_size) != 0)&nbsp;return 1;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trainNetwork(myNN, learning_rate);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;else if (getSteadyState(myNN, numInputs, step_size) != 0)&nbsp;return 1;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;/* ----- save results ----- */

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;return 0;

<br> }

<br> 

<br> 

<br> // getSteadyState() evolves a network to the self-consistent state x_i = f( W_ij x_j ).

<br> 

<br> int getSteadyState(neural_network NN, int numClamped, double StepSize)

<br> {

<br> &nbsp;&nbsp;&nbsp;const double max_mean_sq_diff = 0.001;

<br> &nbsp;&nbsp;&nbsp;const long maxIterations = 1000;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;double diff, sq_diff, input, newOutput;

<br> &nbsp;&nbsp;&nbsp;int iteration, i, j;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;if (numClamped == NN.numNeurons)&nbsp;return 0;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// keep updating the network until it reaches a steady state

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;for (iteration = 1; iteration &lt;= maxIterations; iteration++)&nbsp;&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sq_diff = 0;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = numClamped; i &lt; NN.numNeurons; i++)	{

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input = 0;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (j = 0; j &lt; NN.numNeurons; j++)&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (i != j)&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input += NN.activity[j] * NN.weights[i*NN.numNeurons + j];

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}}

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;newOutput = 1./(1 + exp(-input));

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;diff = newOutput - NN.activity[i];

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sq_diff += diff*diff;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NN.activity[i] *= 1-StepSize;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NN.activity[i] += StepSize * newOutput;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (sq_diff &lt; max_mean_sq_diff * (NN.numNeurons - numClamped))

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 0;

<br> &nbsp;&nbsp;&nbsp;}

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;return 1;

<br> }

<br> 

<br> 

<br> // trainNetwork() updates the weights and biases using the Hebbian rule.

<br> 

<br> void trainNetwork(neural_network NN, double learningRate)

<br> {

<br> &nbsp;&nbsp;&nbsp;int i, j;

<br> &nbsp;&nbsp;&nbsp;

<br> &nbsp;&nbsp;&nbsp;for (i = 0; i &lt; NN.numNeurons; i++)&nbsp;&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;for (j = 0; j &lt; NN.numNeurons; j++)&nbsp;&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;if (i != j)&nbsp;&nbsp;&nbsp;{

<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NN.weights[i*NN.numNeurons + j] += learningRate * NN.activity[i] * NN.activity[j];

<br> &nbsp;&nbsp;&nbsp;}}}

<br> }

<br> </tt>

<P>

<P>  <u>NN.h</u>

<P> 

<P><P><tt> 

<br> typedef struct {

<br> &nbsp;&nbsp;&nbsp;int numNeurons;&nbsp;&nbsp;&nbsp;&nbsp;// 'N'

<br> &nbsp;&nbsp;&nbsp;double *weights;&nbsp;&nbsp;&nbsp;// N x N array of incoming synapses

<br> &nbsp;&nbsp;&nbsp;double *activity;&nbsp;&nbsp;// length-N vector

<br> } neural_network;

<br> 

<br> extern int runNetwork(int, char **);

<br> extern int getSteadyState(neural_network, int, double);

<br> extern void trainNetwork(neural_network, double);

<br> </tt>

<P>

<P> <ul>
</ul><br><P><div align="center"><a href="https://heltilda.github.io/cicada/help_2.html">Prev: Example:  neural networks in Cicada</a> &nbsp;&nbsp;
<a href="https://heltilda.github.io/cicada/help_22.html">Next: Putting the C in Cicada</a>
</div><br><br><P>Last update: July 8, 2018